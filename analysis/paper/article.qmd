---
title: "Hire Ed: Job Market Dynamics for Tenure-Track Faculty Positions in Archaeology"
author:
  - Ben Marwick:
      correspondence: "yes"
      email: bmarwick@uw.edu
      orcid: 0000-0001-7879-4531
      institute:
        - uw
  - Anne Marie Poole:
      institute: uw
      orcid: xxx
  - Ailin Zhang:
      institute: uw
      orcid: xxx
  - Setareh Shafizadeh:
      orcid: 0000-0003-2231-4310
      institute: uw
  - Jess Beck:
      orcid: 0000-0002-7387-2307
      institute:
        - ucd
institute:
  - uw: Department of Anthropology, University of Washington, Seattle, USA
  - ucd: School of Archaeology, University College Dublin, Ireland
  
title-block-published: "Last updated"  
date: now
date-format: long
format: 
  docx:
    reference-doc: "../templates/template.docx" # Insert path for the DOCX file
execute:
  echo: false
  warning: false
  message: false
  comment: "#>"
  fig-path: "../figures/"
  fig-dpi: 900
filters:
  - ../templates/scholarly-metadata.lua
  - ../templates/author-info-blocks.lua
  - ../templates/pagebreak.lua
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Academic careers are frequently sought by archaeology graduate students. Job listing websites often serve as the first place for these students when seeking academic positions. We examined tenure-track job advertisements over the past decade to gain insights into the academic job market for archaeologists. Using data from the community-edited Academic Jobs Wiki for Archaeology, we examine changes in the academic job market over time. We studied the text of 449 job ads posted from 2013-2023. Our analysis focuses on shifts in archaeological topics and methods requested in job ads. We investigate whether the burden on applicants has changed over time: do institutions request more information and documents from applicants at the initial stages of application, compared to a decade ago? We also examine whether there is an increasing trend in job advertisements highlighting diversity and inclusivity, thereby encouraging a broader range of applicants. Additionally, we assess the influence of socio-political factors on the changing focus of research topics in the field. This research aims to assist current and future archaeology students and graduates in better understanding the job market and the requirements of employers, thereby aiding them in effectively preparing for their applications for positions in archaeology.
keywords: |
  keyword 1; keyword 2; keyword 3
highlights: |
  These are the highlights. 
---

<!-- This is the format for text comments that will be ignored during renderings. Do not put R code in these comments because it will not be ignored. -->

<!-- With the following code you can access and display values from the yml header above. -->

Keywords: `r rmarkdown::metadata$keywords`

Highlights: `r rmarkdown::metadata$highlights`

<!-- The actual document text starts here: -->

<!-- Instructors to authors for AAQ 

https://documents.saa.org/container/docs/default-source/doc-publications/style-guide/saa-style-guide_english_updated_2021_final08023c15928949dabd02faafb269fb1c.pdf?sfvrsn=c1f41c1b_11

"AAQ recommends 10,000 words for ARTICLES... Total word count includes
the title page, abstracts (English and second language), keywords (English and second language), text, acknowledgments, funding statements, statements of data availability and competing interests, lists of online supplemental material, figure and table captions, text of all tables (for Excel tables, copy and paste into a Word document to check the word count), notes, and the references cited section"

Our notes document: https://docs.google.com/document/d/12FNfuAPIdZPBlKTEZK_HEWM8GwR_ZHyS/edit Ailin: https://docs.google.com/document/d/13OzXGFdZFraEb60ZXGwMUcRxCRKZxAQG/edit & https://docs.google.com/document/d/1NUF6dD8EnStlM6OameVvI9v6ehtf9IpWNt5SCYZgHPQ/edit

Our SAA poster: Poole et al. https://osf.io/p5uzg & https://docs.google.com/presentation/d/1bljiJiE_JmywxRCaUGh4NkkcaMnQNh2uIBUcmxlp-yw/edit#slide=id.g26d90de87be_2_0

Our URS23 poster: Zhang et al. https://docs.google.com/presentation/d/1u9OGBJNRilNBXAsysiY3z4Q5_qOQa4fxxzBfVlZnQYk/edit#slide=id.g2d1d7d9d51d_0_0 

-->

# Introduction

The aim of this paper is to explore the demand-side of the academic job market for archaeologists in the United States. We had two aims: to determine if disciplinary trends can be discerned in the topical, geographic, and method foci of the positions advertised; and to investigate how the requirements for applicants have changed over time. 

# Background

# Methods

Our primary data source is the Archaeology Academic Jobs Wiki. Originating in 2007, this is a set of freely accessible web pages that anyone can edit (anonymously or with a free user account) hosted by Fandom, a for-profit company. The Archaeology pages are part of the Academic Jobs Wiki, which coordinates similar collaboratively-edited resources for around 40 academic disciplines. The coordinators and contributors are nearly all anonymous or pseudonymous. Typically contributors copy and paste the text of job ads from other sources, such as the _Chronicle of Higher Education_, _Higher Ed Jobs_, and university websites, into the wiki, collecting ads originally posted in numerous different locations. Other contributors then edit the web page to add comments below an ad to share relevant information based on their experience in applying for that position, such as a tally of how many people have applied, the dates of events such as requests for more materials, interviews, offer made, rejection notices, etc. Contributors also edit the page to ask and answer questions about the positions and the application process. These comments make the Academic Jobs Wiki a unique resource for timely and specific information for job-seekers about positions they are interested in, and one of the most important internet resources for the academic job market. Because of its reputation for aggregating ads from diverse sources and rapidly-updated information that is not available elsewhere, the Academic Jobs Wiki has a large community of users that keep it updated and accurate has become an authoritative data source for studies of hiring trends in academia [e.g. @musial2018five] and a widely recommended resource [e.g. @lightfoot2021preparing]. 

For each tenure-track job advertised on the Archaeology Academic Jobs Wiki during 2013-2023 we read the text and recorded into a Google form the name of the hiring institution, the title of the position, and exact words and phrases from the ad about the topical, geographic, and methods foci on the position. The topical focus is what we understood as the primary intellectual focus of the position. The geographic focus is the region of the world that the ideal candidate has scholarly expertise on. The methods focus is the data-generating sub-field of archaeology that is mentioned in the ad. We recorded the type and number of documents requested in each ad (e.g. cover letter, CV, statements on research, teaching, diversity, syllabi, course descriptions, writing samples, transcripts) and how many names/letters of recommenders were requested in the ad.

After completing primary data collection, we studied the topical, geographic, and methods foci of each ad and collaboratively and manually reduced the variation in the raw data into 10-15 categories to simplify analysis and visualisation. Our topic categories were: American archaeology, Ancient Europe and Mediterranean, Archaeological science, Archaeological theory, Biological anthropology, Complex societies, Digital archaeology, Environmental archaeology, Evolutionary anthropology, Indigenous and Historical archaeology, North Mesoamerican Archaeology, Pleistocene archaeology, and Public archaeology Our geographic categories were: Africa, Americas, Asia & India, Canada & Arctic, Europe, Mediterranean, Meso- & South America, Near East, Oceania, Midwest US, Northeastern US, Southeast US, Southwest US, and Western US. Our methods categories were: Archaeobotany, Archaeometry, Bioarchaeology, Ceramic analysis, Computational and Digital archaeology, Geoarchaeology, Landscape analysis, Lithic analysis, Material culture analysis, and Zooarchaeology. Ads could have multiple or none of these three foci, and some of the foci overlap. Some topics include geographic regions because this is how they are typically understood by archaeologists. For example Mesoamerican archaeology is understood to refer to a specific time period and geographic region. While these overlaps can make the data challenging to interpret, in our view it reflects the complex realities of  how search committees express their needs in searching for new faculty, and is insightful in how it reveals intersections between different foci. 

# Results

```{r}
#| label: get-data

library(tidyverse)
library(here)
library(ggbeeswarm)
# This CSV file was downloaded from our data sheet here
# https://docs.google.com/spreadsheets/d/1Jwe3UqJyedrV-QWlwR_44__t4xBVrCfxGyhXdi3E0sg/edit?resourcekey#gid=1686084773
# note that you may need to download it again to get the latest updates!

jobdata <- read_csv(here::here('analysis/data/raw_data/Tenure Track Job Advertisements in Archaeology (Responses) - Form Responses 1.csv')) %>% 
  # simplify the column names 
  janitor::clean_names()

total_number_of_ads_in_our_sample <- nrow(jobdata) # 550

# for text on our figures
base_size <- 12
```

```{r}
#| label: fig-how-many-jobs-per-year

# we can get the year from the URL to the Academic Job Ads Wiki

year_ad_posted <- 
jobdata %>% 
  pull(url_to_data_source_e_g_paste_in_url_to_the_jobs_wiki_page) %>% 
  str_extract(.,  "[[0-9]]{4}-[[0-9]]{4}|2021-22") %>% 
  str_replace("2021-22",  "2021-2022") 
 # fix for 2021-22 DONE!
 # fix for 2023 DONE!

jobdata <- 
  jobdata %>% 
  mutate(year_ad_posted = year_ad_posted) %>% 
  drop_na(year_ad_posted) 

```

```{r}
#| label: fig-how-many-jobs-per-year-by-rank

# how many jobs of each rank per year?

jobdata <-
jobdata %>%
  # simplify rank descriptions
  mutate(title_of_position_tenure_track_jobs_only = tolower(title_of_position_tenure_track_jobs_only)) %>%
  mutate(job_title_simple = case_when(
   str_detect(title_of_position_tenure_track_jobs_only,
              "assistant prof|asst. prof|asst prof") ~ "Assistant Professor",
   str_detect(title_of_position_tenure_track_jobs_only,
              "associate prof|assoc. prof") ~ "Associate Professor",
   str_detect(title_of_position_tenure_track_jobs_only,
              "full prof") ~ "Full Professor",
   str_detect(title_of_position_tenure_track_jobs_only,
              "assistant or associate prof|assistant/associate prof") ~ "Assistant or Associate Professor",
   str_detect(title_of_position_tenure_track_jobs_only,
              "open rank|open-rank|assistant, associate, or full prof|assistant prof, associate prof, or prof") ~ "Open Rank",
   .default = "Other (Curator, Director, etc.)"))

# explore over time
fig_prop_by_job_title_per_year <- 
jobdata %>%
  group_by(year_ad_posted) %>%
  count(job_title_simple) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot() +
  aes(year_ad_posted,
      n,
      group = job_title_simple,
      fill = job_title_simple) +
 # geom_line(linewidth = 2) +
  geom_col() +
 theme_minimal(base_size = base_size) +
  xlab("") +
  ylab("Number of\ntenure track job ads") +
  scale_fill_brewer(palette = "Dark2") +
  guides(fill = guide_legend(nrow=2,
                               byrow=TRUE,
                               "Job title",
                                keywidth=0.1,
                 keyheight=0.1,
                 default.unit="inch")) +
    theme(legend.position = c(0.5, 0.85)) +
  guides(x = "none")

```

```{r}
#| label: ratio-tt-to-non-tt

base_url_to_2019 <- "https://academicjobs.fandom.com/wiki/Archaeology_Jobs_"
base_url_after_2020 <- "https://academicjobs.fandom.com/wiki/Archaeology_"

# starts at 2010-2011
# fix for 2021-22
# base UR

years_to_2019 <- map_chr(2012:2019, ~str_glue('{.x}-{.x +1}'))
years_after_2020 <- map_chr(2020:2022, ~str_glue('{.x}-{.x +1}'))
# though it seems to start at 2007-8: https://academicjobs.fandom.com/wiki/Archaeology_07-08

# make a set of URLs for each page for each year
urls_for_each_year <- c(str_glue('{base_url_to_2019}{years_to_2019}'), 
                        str_glue('{base_url_after_2020}{years_after_2020}')) %>% 
    str_replace("2021-2022", "2021-22")
```

```{r}
#| eval: false

library(rvest)

# scrape the web pages to get the 
# ratio of tenure-track to untenured positions
# base URL changes after 2018_2019
# We did this once and then saved the output so we don't scrape every
# time the quarto document is rendered. So this code block is
# eval: false to avoid repeated and unnesecary scraping

# all years
urls_for_each_year_headers <- 
map(urls_for_each_year,
    ~.x %>% 
      read_html() %>% 
      html_nodes('.mw-headline') %>% 
      html_text())

# save this list so we don't have to scrape again
library(rlist)
list.save(urls_for_each_year_headers, 
          here('analysis/data/raw_data/urls_for_each_year_headers.yaml'))
```

```{r}
#| label: ratio-tt-to-non-tt-jobs

library(rlist)
urls_for_each_year_headers <-  list.load( here('analysis/data/raw_data/urls_for_each_year_headers.yaml'))

# keep only headings that are actual jobs, they include the terms:
job_headings <- c("college", "university")

total_number_of_jobs_per_year <- 
  map(urls_for_each_year_headers,
      ~str_subset(tolower(.x),
          paste0(job_headings, collapse = "|")))

total_number_of_jobs_per_year_n <- 
map_int(total_number_of_jobs_per_year, length)

total_number_of_jobs_per_year_tbl <- 
tibble(
  url_to_data_source_e_g_paste_in_url_to_the_jobs_wiki_page = urls_for_each_year,
  total_number_of_jobs_per_year = total_number_of_jobs_per_year_n
)

# count of TT jobs per year from our manual data collection,
# join with our total number of all jobs by scraping
count_of_tt_jobs_per_year_from_our_form <- 
jobdata %>% 
  group_by(url_to_data_source_e_g_paste_in_url_to_the_jobs_wiki_page) %>% 
  tally() %>% 
  right_join(total_number_of_jobs_per_year_tbl) %>% 
  rename(n_tt_jobs  = n,
         n_total_jobs = total_number_of_jobs_per_year) %>% 
  mutate(n_non_tt_jobs = n_total_jobs - n_tt_jobs,
         ratio_tt_2_ntt = n_tt_jobs / n_non_tt_jobs) %>% 
  mutate(year = str_extract(url_to_data_source_e_g_paste_in_url_to_the_jobs_wiki_page,  "[[0-9]]{4}-[[0-9]]{4}|2021-22")) %>%  
  mutate(year = ifelse(year =="2021-22",  "2021-2022", year))  %>% 
  # add line break for prettier plotting
  mutate(year_ad_posted_break = str_replace(year, "-", "-\n"))
  

# draw plot
fig_ratio_tt_2_ntt_jobs_per_year <- 
  ggplot(count_of_tt_jobs_per_year_from_our_form) +
  aes(year_ad_posted_break, 
      group = 1,
      ratio_tt_2_ntt) +
  #geom_line(linewidth = 2) +
  geom_col() +
  geom_hline(yintercept = 1,
             colour = "red") +
  annotate("text", 
           x = 6, 
           y = 1.4, 
           label = "1:1 ratio",
           colour = "red",
           size = 3) +
  labs(y = "Ratio of tenure-track\nto non-tenure track and other",
       x = "") +
  theme_minimal(base_size = base_size) +
  scale_x_discrete(name = "Academic Year") 
   #guides(x = "none")
  

```

```{r}
#| label: combine-basic-plots

# save these three plots as one set
library(cowplot)
p <- 
plot_grid(
  fig_prop_by_job_title_per_year,
  fig_ratio_tt_2_ntt_jobs_per_year,

  ncol = 1,
  align = "hv",
  axis = "lr",
  labels = "AUTO",
  label_size = 8
)

ggsave(filename = here("analysis",
            "figures", 
            "fig-panel-per-year.png"),
       bg ="white",
      plot = p,
       h = 15, # experiment with h and w to get the right size and proportion 
       w = 20,
       units = "in",
       dpi = 900) # make the image nice and crisp

```

```{r}
#| label: fig-show-basic-plots
#| fig-cap: "A: total number of job ads posted to the Academic Jobs Wiki for Archaeology in each year, with coloured sections showing the proportion of jobs by title and rank. B: Ratio of tenure-track to non-tenure-track positions over time."

# show figure from PNG file
knitr::include_graphics(here("analysis",
            "figures", 
            "fig-panel-per-year.png"))
```

We collected data from `r total_number_of_ads_in_our_sample` ads for tenure-track jobs in archaeology posted during 2013-2023. @fig-show-basic-plots shows the count of ads for each year. Assistant Professor jobs are consistently the most common title and rank, and open rank or full professor are the least frequent. The ratio of tenure-track to non-tenure track positions is generally well above one.  Only 2013-2014 had more non-tenure track positions than tenure track, which was followed by an upward trend peaking at 2018-2019 and the declining again into the present.

## Characteristics of the hiring institutions

```{r}
# Draw of map to show which states have done the most hiring in our sample

# get the text in parentheses after the university name that gives the
# state or country abb
uni_state_country <- # 550
jobdata %>%  # 550 rows
 select(name_of_hiring_university) %>%
  mutate(state_country = regmatches(name_of_hiring_university,
                                    gregexpr( "(?<=\\().+?(?=\\))",
                                              name_of_hiring_university,
                                              perl = T))) %>%
  unnest(state_country)

# tally to get counts:
uni_state_country_tally <-
uni_state_country %>%
  group_by(state_country) %>%
  tally(sort = TRUE)

# did we get all the job ads?
# sum(uni_state_country_tally$n) # 550 all of them

state_to_st <- function(x){
       c(state.abb, 'DC')[match(x, c(state.name, 'District of Columbia'))]
}

state_name_and_abb <-
enframe(state.name, value = 'state_name') %>%
       mutate(state_abbr = state_to_st(state_name))

# filter to get US states only
uni_state_country_tally_us <-
uni_state_country_tally %>%
  filter(state_country %in% state.abb) %>%
  select(state_abbr = state_country, n) %>%
  # make sure we have all states in the dataframe
  # even those with no jobs
  right_join(state_name_and_abb) %>%
  select(state = state_name, n, state_abbr) %>%
  mutate(state = tolower(state)) %>%
  mutate(n = ifelse(is.na(n), 0, n))

# how many jobs ads now?
# sum(uni_state_country_tally_us$n) # 433, 78% of the total

library(ggplot2)
library(fiftystater)
library(tidyverse)
library(ggrepel)

data("fifty_states")

p_map <- 
ggplot(data= uni_state_country_tally_us,
       aes(map_id = state)) +
  geom_map(aes(fill = n),
           color= "black",
           linewidth = 0.1,
           map = fifty_states) +
  expand_limits(x = fifty_states$long,
                y = fifty_states$lat) +
  coord_map() +
  geom_text_repel(data = fifty_states %>%
              group_by(id) %>%
              summarise(lat = mean(c(max(lat), min(lat))),
                        long = mean(c(max(long), min(long)))) %>%
              mutate(state = id) %>%
              left_join(uni_state_country_tally_us,
                        by = "state"),
            aes(x = long,
                y = lat,
                label = n,
                bg.color = "white",
                bg.r = 0.1),
            force = 0,
            force_pull = 100,
            size = 3)+
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL) +
  labs(x = "",
       y = "") +
  theme(legend.position = "bottom",
        panel.background = element_blank()) +
  scale_fill_viridis_c()

ggsave(plot = p_map, 
       filename = here("analysis",
            "figures",
            "fig-us-state-map.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion
       w = 12,
       units = "in",
       dpi = 900) # make the image nice and crisp))
```

```{r}
# Carnegie Classification
library(stringi)
library(readxl)

# data from https://carnegieclassifications.acenet.edu/resource/2021-update-public-file/
CC <-
  read_excel(here::here('analysis/data/raw_data/CCIHE2021-PublicData.xlsx'),
              sheet = "Values1")
Basic2021 <-
  read_excel(here::here('analysis/data/raw_data/CCIHE2021-PublicData.xlsx'),
             sheet = "Data1") 
  # remove a bunch of text from uni names to improve our joins

uni_name <- # 547
 jobdata %>%  # 547 rows
  # remove parentheses and their contents
  mutate(name = str_squish(str_replace(name_of_hiring_university,
                            "\\s*\\(.*?\\)$", 
                            ""))) %>% 
  mutate(name = str_replace(name,
                            "California State University,[[:space:]]",
                            "California State University-")) %>% 
 
  left_join(Basic2021, 
            keep = TRUE ) 

# how many schools did we match with the CC data
uni_name %>% 
  filter(!is.na(basic2021)) %>% 
  nrow() # 245, 290, 300, 313, 404, 430 / 433 for US unis

# which schools don't match with the CC data?
no_match <- 
uni_name %>% 
  select(name.x, name.y)  %>% 
  filter(is.na(name.y)) %>% 
  distinct() 

uni_name_tally <- 
uni_name %>% 
  left_join(CC,
            join_by("basic2021" == "Value")) %>% 
  group_by(Category) %>% 
  tally(sort = TRUE) %>% 
  drop_na()

p_c <- 
uni_name_tally %>% 
  mutate(Category = str_wrap(Category, width = 30)) %>% 
ggplot() +
  aes(reorder(Category, n), n) +
  geom_col() +
  coord_flip() +
  xlab("") +
  theme_minimal(base_size = 14)

library(cowplot)
p_map_c <- 
ggdraw(p_c) +
  draw_plot(p_map, .4, .1, .65, .65) +
  draw_plot_label(
    c("A", "B"),
    c(0, 0.45),
    c(1, 0.65),
    size = 12
  )

ggsave(here("analysis",
            "figures", 
            "fig-map-and-carnegie-classification.png"),
       plot = p_map_c,
       bg ="white",
       h = 7, # experiment with h and w to get the right size and proportion
       w = 12,
       units = "in",
       dpi = 900) # make the image nice and crisp))

```

```{r}
#| label: fig-show-map-of-hiring-institution-and-cc
#| fig-cap: "A: Frequency of hiring institution by Carnegie classification. B: Inset shows map of the United States showing the count of tenure-track job ads posted by all insititutions in each state during 2013-2023"

# show figure from PNG file
knitr::include_graphics(here("analysis",
            "figures",
            "fig-map-and-carnegie-classification.png"))
```

Panel A of @fig-show-map-of-hiring-institution-and-cc shows the frequencies of institutions according to their Carnegie Classification, which is a framework for classifying US colleges and universities according to the types of degrees awarded, levels of activity such as research, and topical foci. Doctoral universities with high and very high research activity are by far the most active with hiring archaeology faculty. Associate's colleges, also known as community colleges, rarely post job ads for archaeology faculty.  

Panel B of @fig-show-map-of-hiring-institution-and-cc shows the geographic distribution of the hiring institutions. California posted almost twice as many job ads as the next most active states. After California, the states that posted the most ads during 2013-2023 include New York, Texas, and Pennsylvania, and Florida. These top five states also correspond to the top five most populous US states, indicating that rates of hiring is approximately proportional to population density. Similarly, the lowest counts of job ads were observed in states with the lowest populations: North Dakota, South Dakota, Alaska, and Nebraska. No institutions in Montana posted a job ad during this period.

## Geographic trends over time in job ads


```{r}
#| label: fig-geographic-focus-by-year 

# geographic focus by year

library(stringi)

geographic_foci <-
  read_csv(here("analysis/data/raw_data/Tenure Track Job Advertisements in Archaeology - geography.csv"))

geographic_foci_clean <-
  map(
   str_split(geographic_foci$`From the data`, ";"),
   ~.x %>%
     str_squish() %>%
     stri_remove_empty())

jobdata_geo <-
  jobdata %>%
  select(geographic_focus_of_position)

jobdata_geo <-
  # add one column for each geo region in our categories
cbind(jobdata_geo,
      setNames( lapply(geographic_foci$Category2, function(x) x=NA),
                geographic_foci$Category2) )

for(i in 1:length(geographic_foci$Category2)){

  this_location <- geographic_foci$Category2[i]

  # create the pattern to search for
  x <- paste0(geographic_foci_clean[[i]], collapse = "|")

  # do the search through all the job ads for that pattern
  y <- str_detect(jobdata_geo$geographic_focus_of_position,
             x)

  # assign back to our data frame in the appropriate location column
  jobdata_geo[, this_location] <- y

}

# BM TODO: check for job ads that have a location, but we're not getting it

united_states_regions <-
str_subset(geographic_foci$Category2, "US")

jobdata_geo_year <-
jobdata %>%
  bind_cols(jobdata_geo) %>%
  select(year_ad_posted,
         geographic_foci$Category2) %>%
  pivot_longer(-year_ad_posted) %>%
  drop_na()

# how many times each location mentioned?
jobdata_geo_year %>%
  group_by(name) %>%
  summarise(n = sum(value)) %>%
  arrange(desc(n)) %>%
  ggplot() +
  aes(reorder(name, n),
      n)+
  geom_col() +
  xlab("") +
  theme_minimal() +
  coord_flip()

# explore trends over time. put a point on the max year
jobdata_geo_year_tally <-
jobdata_geo_year %>%
 # exclude those with <20 ads
  filter(!name %in% c("Canada & Arctic",
                      "Oceania",
                      "Southeast US",
                      "Southwest US",
                      "Western US",
                      "Midwest US",
                      "Northeastern US"
                      )) %>%
  group_by(year_ad_posted,
           name) %>%
  summarise(n = sum(value)) %>%
  mutate(prop = n / sum(n))

jobdata_geo_year_tally_max <-
  jobdata_geo_year_tally %>%
  group_by(
           name ) %>%
  filter(prop == max(prop))

library(ggrepel)

ggplot() +
  geom_smooth(data =  jobdata_geo_year_tally,
            aes(year_ad_posted,
                 prop,
                 group = name,
                 colour = name),
            size = 3,
           se = FALSE 
           ) +
  xlab("Year") +
  ylab("Proportion of all ads") +
  scale_colour_brewer(palette = "Dark2") +
  guides(colour = guide_legend("Geographic\nfocus",
                               label.position = "bottom")) +
    theme_minimal( base_size = base_size) +
  theme(legend.position="bottom") 

ggsave(here("analysis",
            "figures", 
            "fig-geo-focus-by-year.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion 
       w = 20,
       units = "in",
       dpi = 900) # make the image nice and crisp)


# what about within the US
# how many times each location mentioned?
jobdata_geo_year %>%
  group_by(name) %>%
  summarise(n = sum(value)) %>%
  arrange(desc(n)) %>%
  filter(name %in% united_states_regions) %>% 
  ggplot() +
  aes(reorder(name, n),
      n)+
  geom_col() +
  ylab("Number of ads") +
  xlab("") +
  theme_minimal(base_size = 24) +
  coord_flip()

ggsave(here("analysis",
            "figures", 
            "fig-geo-us-focus-by-year.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion 
       w = 20,
       units = "in",
       dpi = 900) # make the image nice and crisp)


```


- what regions of the world are mentioned in the ads?


- basic trends
- where are the hiring universities?
- what Carnegie classifications of the hiring universities?



## Topic trends over time

## Method trends over time

## Instructions to applicants over time





```{r}
# stop rendering at this point because the code below is not working
knitr::knit_exit()
```

```{r}
#| label: fig-requirements-over-time

# look at only these requirements because the others are flat

intresting_requirements <- 
c("cover letter",
  "cv",
  "names of recommenders",
  "diversity statement",
  "research statement",
  "teaching statement")

jobdata_requirements <- 
jobdata %>% 
  select(year_ad_posted,
         starts_with("documents_requested")) %>% 
  pivot_longer(-year_ad_posted) %>% 
  mutate(value = case_when(
    value == "Not requested in the job ad" ~ 0,
    value == "One" ~ 1,
    value == "Two (e.g. two syllabi)" ~ 2,
    value == "Three" ~ 3,
    .default = 0
  ))  %>% 
  # trim names a bit
  mutate(name = str_remove(name, "documents_requested_")) %>% 
  mutate(name = str_replace_all(name, "_", " ")) %>% 
  filter(name %in% intresting_requirements) %>% 
  mutate(name = str_wrap(name, 10),
         year_ad_posted = str_replace(year_ad_posted, "-", "\n"))

jobdata_requirements_means <- 
jobdata_requirements %>% # average number requested per year
  group_by(year_ad_posted, 
           name) %>% 
  summarise(mean_n = mean(value))

integer_breaks <- function(n = 5, ...) {
  fxn <- function(x) {
    breaks <- floor(pretty(x, n, ...))
    names(breaks) <- attr(breaks, "labels")
    breaks
  }
  return(fxn)
}


ggplot(jobdata_requirements_means) +
  aes(year_ad_posted, 
      mean_n,
      group = name) +
  geom_smooth(linewidth = 2,
              colour = "black") +
  geom_jitter(data = jobdata_requirements,
             aes(year_ad_posted, 
                 value),
             alpha = 0.1,
             height = 0.2,
             width =  0.1) +
  facet_wrap(~name,
             scales = "free_y",
             nrow = 1) +
  xlab("Year") +
  ylab("Number requested in job ad") +
  scale_y_continuous(breaks = integer_breaks()) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(size = 8),
        strip.text = element_text( size = 20))

ggsave(here("analysis",
            "figures", 
            "fig-requirements-per-year.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion 
       w = 20,
       units = "in",
       dpi = 900) # make the image nice and crisp
  
```



```{r}
#| label: fig-requirements-by-position 

# do the requirements differ for associate positions 
jobdata_requirements_by_rank <- 
jobdata %>% 
  mutate(position_title = case_when(
    str_detect(title_of_position_tenure_track_jobs_only, 
               "associate") ~ "associate",
    str_detect(title_of_position_tenure_track_jobs_only, 
               "assistant") ~ "assistant",
    str_detect(title_of_position_tenure_track_jobs_only, 
               "full") ~ "full"))  %>% 
  select(position_title,
         starts_with("documents_requested")) %>% 
  pivot_longer(-position_title) %>% 
  mutate(value = case_when(
    value == "Not requested in the job ad" ~ 0,
    value == "One" ~ 1,
    value == "Two (e.g. two syllabi)" ~ 2,
    value == "Three" ~ 3,
    .default = 0
  )) %>% 
  filter(!is.na(position_title)) 

jobdata_requirements_by_rank_means <- 
  jobdata_requirements_by_rank %>% 
  group_by(position_title,
           name) %>% 
  summarise(mean = mean(value))

ggplot() +
    geom_jitter(data = jobdata_requirements_by_rank,
                  aes(position_title, 
                      value),
                height = 0.05,
                alpha = 0.1) +
    geom_point(data = jobdata_requirements_by_rank_means,
               aes(position_title,
                   mean),
               size = 4,
               colour = "red") +
  facet_wrap( ~ name,
              scales = "free_y") +
  theme_minimal()
```





```{r}

# topical focus by year

library(googlesheets4)
library(stringi)

topical_foci <-
  read_sheet("https://docs.google.com/spreadsheets/d/1AHq49pIyChcgJ7rawe6KMWkdIBXydCamvg8Jslob8Ec/edit#gid=0",
             sheet = "topic")

topical_foci_clean <-
  map(
    str_split(topical_foci$`From the data`, ";"),
    ~.x %>%
      str_squish() %>%
      stri_remove_empty() %>%
      str_to_lower)

jobdata_topic <-
  jobdata %>%
  select(topical_focus_of_position) %>%
  mutate(topical_focus_of_position = str_to_lower(topical_focus_of_position))

jobdata_topic <-
  # add one column for each topic in our categories
  cbind(jobdata_topic,
        setNames( lapply(topical_foci$Category, function(x) x=NA),
                  topical_foci$Category) )

for(i in 1:length(topical_foci$Category)){

  this_topic <- topical_foci$Category[i]

  # create the pattern to search for
  x <- paste0(topical_foci_clean[[i]], collapse = "|")

  # do the search through all the job ads for that pattern
  y <- str_detect(jobdata_topic$topical_focus_of_position,
                  x)

  # assign back to our data frame in the appropriate location column
  jobdata_topic[, this_topic] <- y

}

jobdata_topic_year <-
  jobdata %>%
  bind_cols(jobdata_topic) %>%
  select(year_ad_posted,
         topical_foci$Category) %>%
  pivot_longer(-year_ad_posted) %>%
  drop_na()

# how many times each topic mentioned?
jobdata_topic_year %>%
  group_by(name) %>%
  summarise(n = sum(value)) %>%
  arrange(desc(n)) %>%
  ggplot() +
  aes(reorder(name, n),
      n)+
  geom_col() +
  xlab("") +
  theme_minimal() +
  coord_flip()

# explore trends over time. put a point on the max year
jobdata_topic_year_tally <-
  jobdata_topic_year %>%
  # exclude those with <20 ads
  filter(!name %in% c("Digital Archaeology",
                      "Pleistocene archaeology",
                      "Mesoamerican Archaeology",
                      "Biological anthropology",
                      "Archaeological theory",
                      "Evolutionary anthropology",
                      "North American archaeology"
  )) %>%
  group_by(year_ad_posted,
           name) %>%
  summarise(n = sum(value)) %>%
  mutate(prop = n / sum(n))

jobdata_topic_year_tally_max <-
  jobdata_topic_year_tally %>%
  group_by(
    name ) %>%
  filter(prop == max(prop))

library(ggrepel)

ggplot() +
  geom_smooth(data =  jobdata_topic_year_tally,
            aes(year_ad_posted,
                prop,
                group = name,
                colour = name),
            size = 3,
           span = 0.7,
            se = FALSE) +
  xlab("Year") +
  ylab("Proportion of all ads") +
  scale_colour_brewer(palette = "Dark2") +
  guides(colour = guide_legend("Topic\nfocus",
                               label.position = "bottom")) +
    theme_minimal( base_size = 28) +
  theme(legend.position="bottom") 

ggsave(here("analysis",
            "figures", 
            "fig-topic-focus-by-year.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion
       w = 20,
       units = "in",
       dpi = 900) # make the image nice and crisp))

```


```{r}
# method focus by year

library(googlesheets4)
library(stringi)

method_foci <-
  read_sheet("https://docs.google.com/spreadsheets/d/1AHq49pIyChcgJ7rawe6KMWkdIBXydCamvg8Jslob8Ec/edit#gid=0",
             sheet = "method")

method_foci_clean <-
  map(
    str_split(method_foci$`From the data`, ";"),
    ~.x %>%
      str_squish() %>%
      stri_remove_empty() %>%
      str_to_lower)

jobdata_method <-
  jobdata %>%
  select(methods_focus_of_position) %>%
  mutate(methods_focus_of_position = str_to_lower(methods_focus_of_position))

jobdata_method <-
  # add one column for each topic in our categories
  cbind(jobdata_method,
        setNames( lapply(method_foci$Category, function(x) x=NA),
                  method_foci$Category) )

for(i in 1:length(method_foci$Category)){

  this_method <- method_foci$Category[i]

  # create the pattern to search for
  x <- paste0(method_foci_clean[[i]], collapse = "|")

  # do the search through all the job ads for that pattern
  y <- str_detect(jobdata_method$methods_focus_of_position,
                  x)

  # assign back to our data frame in the appropriate location column
  jobdata_method[, this_method] <- y

}

jobdata_method_year <-
  jobdata %>%
  bind_cols(jobdata_method) %>%
  select(year_ad_posted,
         method_foci$Category) %>%
  pivot_longer(-year_ad_posted) %>%
  drop_na()

# how many times each topic mentioned?
jobdata_method_year %>%
  group_by(name) %>%
  summarise(n = sum(value)) %>%
  arrange(desc(n)) %>%
  ggplot() +
  aes(reorder(name, n),
      n)+
  geom_col() +
  xlab("") +
  theme_minimal() +
  coord_flip()

# explore trends over time. put a point on the max year
jobdata_method_year_tally <-
  jobdata_method_year %>%
  # exclude those with <20 ads
  filter(!name %in% c("Material culture analysis",
                      "Ceramic analysis"
  )) %>%
  group_by(year_ad_posted,
           name) %>%
  summarise(n = sum(value)) %>%
  mutate(prop = n / sum(n))

library(ggrepel)

ggplot() +
  geom_smooth(data =  jobdata_method_year_tally,
            aes(year_ad_posted,
                prop,
                group = name,
                colour = name),
            size = 3,
           span = 0.7,
            se = FALSE) +
  xlab("Year") +
  ylab("Proportion of all ads") +
  guides(colour = guide_legend("Method\nfocus",
                               label.position = "bottom")) +
    theme_minimal( base_size = 28) +
  scale_colour_brewer(palette = "Dark2") +
  theme(legend.position="bottom") 

ggsave(here("analysis",
            "figures", 
            "fig-method-focus-by-year.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion
       w = 20,
       units = "in",
       dpi = 900) # make the image nice and crisp))
```

```{r}
# EEO
library(stringr)
library(dplyr)
library(stopwords)
library(tidyverse)
library(tidytext)
library(tm)

stopwords <- stopwords("en")

EEO <- jobdata %>%
  select(equal_employment_opportunity_statement, year_ad_posted) %>% 
  drop_na

remove_stopwords <- function(text, stopwords) {
  pattern <- paste0("\\b(", paste(stopwords, collapse = "|"), ")\\b")
  str_remove_all(text, pattern)
}

EEO_clean <- EEO %>%
  mutate(equal_employment_opportunity_statement = str_to_lower(equal_employment_opportunity_statement), 
         equal_employment_opportunity_statement = str_remove_all(equal_employment_opportunity_statement, "[[:punct:]]"),
         equal_employment_opportunity_statement = remove_stopwords(equal_employment_opportunity_statement, stopwords))

#word count for EEO statement
EEO_wordcount <- EEO %>%
  mutate(word_count = str_count(equal_employment_opportunity_statement, "\\S+")) %>%
  print()

jobdata_EEO_year_tally <-
  EEO_wordcount %>%
  group_by(year_ad_posted) %>%
  summarise(n = mean(word_count))

fig_ave_length_EEO_per_year <- 
ggplot(jobdata_EEO_year_tally) +
  aes(year_ad_posted,n) +
  geom_col() +
  ylab("Average length of EEO statement by year") +
  theme_minimal(base_size = 28) +
  guides(x = "none")

ggplot(EEO_wordcount) +
  aes(year_ad_posted, word_count) +
  geom_boxplot() +
  geom_quasirandom()

ggplot(jobdata_requirements_means %>% 
         filter(name == "diversity\nstatement") ) +
  aes(year_ad_posted, 
      mean_n) +
  geom_col() 


  
# here's the list of words of interest
community_word <- c("women","color","religion","race","age","veteran","disability","citizenship","national origin","sex","gender","marital status","genetic information","breastfeeding","sexual orientation","transgender status","gender expression","gender identity","ancestry","income assignment for child support","arrest and court record","domestic or sexual violence victim status","national guard absence","visible minorities","aboriginal persons","indigenous","LGBTQ2S+","pregnancy","childbirth","faith","genetic or family medical history","medical condition","nationality","ethnicity","parental status","accent","ethnic","creed","political affiliation","immigration status")

EEO_word <- EEO_clean %>%
   unnest_tokens(word, 
                 equal_employment_opportunity_statement) %>%
  anti_join(get_stopwords()) %>%
  count(word, sort = TRUE) %>%
  drop_na() 


# prepare a dataframe to store the count of words of interest
df <- data.frame(matrix(ncol = length(community_word), 
                        nrow = 0))
colnames(df) <- community_word

for(i in 1: nrow(EEO_clean)){
  df[i, ] <- str_detect(EEO_clean[i, ], 
                   community_word)
}

# detect if the words of interest is present or not.
EEO_clean_community_word <- 
  bind_cols(EEO_clean, df) %>% 
  rowwise() %>% 
  mutate(community_word_tally = sum(c_across(all_of(community_word)), 
                                    na.rm = T)) %>% 
  bind_cols(EEO_wordcount) %>% 
  mutate(eeo_words_total_ratio = community_word_tally / word_count)

ggplot(EEO_clean_community_word) +
  aes(community_word_tally,
      word_count,
      size = eeo_words_total_ratio) +
  geom_point()

ggplot(EEO_clean_community_word) +
  aes(community_word_tally) +
  geom_histogram()
```



```{r}
ggplot(EEO_word %>% 
         filter(n > 20), 
       aes(x = reorder(word, n), 
           y = n)) +
  geom_col() +
  labs(x = "Words", 
       y = "Frequency", 
       title = "Word Frequency") +
  coord_flip()
```



```{r}
# explore QS rankings

qs_rankings <- 
  readxl::read_excel("analysis/data/raw_data/2024 QS WUR by Subject - Public Results (for qs.com)_48.xlsx", sheet = "Archaeology", skip = 12)

jobdata %>% 
  mutate(Institution = str_remove(name_of_hiring_university,
                                  " \\(.*\\)")) %>% 
  mutate(Institution = case_when(
    # modify the school name to match with the name in the QS list
    Institution == "Ohio State University-Main Campus" ~ "The Ohio State University",
  "Cambridge" ~ "University of Cambridge",
  "Cambridge University" ~ "University of Cambridge",
  "Keble College" ~ "University of Oxford",
  "University of California-Berkeley" ~ "University of California, Berkeley (UCB)",
"University of California-Los Angeles" ~ "University of California, Los Angeles (UCLA)",
"University of Tuebingen" ~ "Eberhard Karls Universität Tübingen",
"Universität Tübingen" ~ "Eberhard Karls Universität Tübingen",
"Arizona State University Campus Immersion" ~ "Arizona State University",
"Kiel University" ~ "Christian-Albrechts-University zu Kiel",
"University of Arizona" ~ "The University of Arizona",
"The Pennsylvania State University" ~ "Pennsylvania State University",
"University of Bologna" ~ "Alma Mater Studiorum - University of Bologna",
"University of Sydney" ~ "The University of Sydney",
"University of Copenhagen, Saxo Institute" ~ "University of Copenhagen",
"The University of Texas at Austin" ~ "University of Texas at Austin",

    .default = as.character(Institution)
  )) %>% 
  select(Institution) %>% 
  left_join(qs_rankings) %>% 
  drop_na(`2024`) %>% 
  distinct(Institution, .keep_all = TRUE) %>% 
  View()


```

# Discussion

# Conclusion

# Acknowledgements

<!-- The following line inserts a page break  -->

\newpage

# References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r}
#| label: colophon
#| cache: false

# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())  
```

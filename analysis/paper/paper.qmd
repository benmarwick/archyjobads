---
title: "Title Goes Here"
author:
  - Jane Doe:
      correspondence: "yes"
      email: janedoe@fosg.org
      orcid: 0000-0003-1689-0557
      institute:
        - fosg
        - fop
  - John Q. Doe:
      institute: fosg
      orcid: 0000-0003-1689-0558
  - Peder Ås:
      institute: fosg
      orcid: 0000-0003-1689-0559
  - Juan Pérez:
      orcid: 0000-0003-1689-0551
      institute:
        - name: Acme Corporation
  - Max Mustermann:
      orcid: 0000-0003-1689-0552
institute:
  - fosg:
      name: Formatting Open Science Group
      address: 23 Science Street, Eureka, Mississippi, USA
  - fop: Federation of Planets
title-block-published: "Last updated"  
date: now
date-format: long
format: 
  docx:
    reference-doc: "../templates/template.docx" # Insert path for the DOCX file
execute:
  echo: true
  warning: false
  message: false
  comment: "#>"
  fig-path: "../figures/"
  fig-dpi: 600
filters:
  - ../templates/scholarly-metadata.lua
  - ../templates/author-info-blocks.lua
  - ../templates/pagebreak.lua
bibliography: references.bib
csl: "../templates/journal-of-archaeological-science.csl" # Insert path for the bib-style
abstract: |
  Text of abstract
keywords: |
  keyword 1; keyword 2; keyword 3
highlights: |
  These are the highlights. 
---

<!-- This is the format for text comments that will be ignored during renderings. Do not put R code in these comments because it will not be ignored. -->

<!-- With the following code you can access and display values from the yml header above. -->

Keywords: `r rmarkdown::metadata$keywords`

Highlights: `r rmarkdown::metadata$highlights`

<!-- The actual document text starts here: -->

# Introduction

Here is a citation [@Marwick2017]

# Background

# Methods

# Results

<!-- Here's some example analysis code: -->

```{r}
#| label: get-data

library(tidyverse)
library(here)
library(ggbeeswarm)
# This CSV file was downloaded from our data sheet here
# https://docs.google.com/spreadsheets/d/1Jwe3UqJyedrV-QWlwR_44__t4xBVrCfxGyhXdi3E0sg/edit?resourcekey#gid=1686084773
# note that you may need to download it again to get the latest updates!

jobdata <- read_csv(here::here('analysis/data/raw_data/Tenure Track Job Advertisements in Archaeology (Responses) - Form Responses 1.csv')) %>% 
  # simplify the column names 
  janitor::clean_names()

total_number_of_ads_in_our_sample <- nrow(jobdata) # 550
```

We have `r total_number_of_ads_in_our_sample` job advertisements in our sample

```{r}
#| label: fig-how-many-jobs-per-year
#| fig-cap: "Number of tenure-track archaeology faculty job ads posted each year"

# we can get the year from the URL to the Academic Job Ads Wiki

year_ad_posted <- 
jobdata %>% 
  pull(url_to_data_source_e_g_paste_in_url_to_the_jobs_wiki_page) %>% 
  str_extract(.,  "[[0-9]]{4}-[[0-9]]{4}|2021-22") %>% 
  str_replace("2021-22",  "2021-2022")
 # fix for 2021-22 DONE!
 # fix for 2023 DONE!

jobdata <- 
  jobdata %>% 
  mutate(year_ad_posted = year_ad_posted) %>% 
  drop_na(year_ad_posted)

fig_how_many_jobs_per_year <- 
ggplot(jobdata) +
  aes(year_ad_posted) +
  geom_bar() +
  scale_x_discrete(name = "") +
  ylab("Number of\ntenure track job ads") +
  theme_minimal(base_size = 14) +
  guides(x = "none")


```

@fig-how-many-jobs-per-year shows how many jobs per year in our sample

```{r}
#| label: fig-how-many-jobs-per-year-by-rank
#| fig-cap: "Proportion of tenure-track archaeology faculty jobs per year by rank"

# how many jobs of each rank per year?

jobdata <-
jobdata %>%
  # simplify rank descriptions
  mutate(title_of_position_tenure_track_jobs_only = tolower(title_of_position_tenure_track_jobs_only)) %>%
  mutate(job_title_simple = case_when(
   str_detect(title_of_position_tenure_track_jobs_only,
              "assistant prof|asst. prof|asst prof") ~ "Assistant Professor",
   str_detect(title_of_position_tenure_track_jobs_only,
              "associate prof|assoc. prof") ~ "Associate Professor",
   str_detect(title_of_position_tenure_track_jobs_only,
              "full prof") ~ "Full Professor",
   str_detect(title_of_position_tenure_track_jobs_only,
              "assistant or associate prof|assistant/associate prof") ~ "Assistant or Associate Professor",
   str_detect(title_of_position_tenure_track_jobs_only,
              "open rank|open-rank|assistant, associate, or full prof|assistant prof, associate prof, or prof") ~ "Open Rank",
   .default = "Other (Curator, Director, etc.)"))

# explore over time
fig_prop_by_job_title_per_year <- 
jobdata %>%
  group_by(year_ad_posted) %>%
  count(job_title_simple) %>%
  mutate(prop = n / sum(n)) %>%
  ggplot() +
  aes(year_ad_posted,
      prop,
      group = job_title_simple,
      colour = job_title_simple) +
  geom_line(linewidth = 2) +
  theme_minimal(base_size = 14) +
  xlab("") +
  ylab("Proportion of\nall tenure-track ads") +
  theme(legend.position = c(0.5, 0.5)) +
  scale_colour_brewer(palette = "Dark2") +
  guides(colour = guide_legend(nrow=2,
                               byrow=TRUE,
                               "Job title")) +
  guides(x = "none")


```


```{r}
#| label: ratio-tt-to-non-tt

# ratio of tenure-track to untenured positions
# base URL changes after 2018_2019

base_url_to_2019 <- "https://academicjobs.fandom.com/wiki/Archaeology_Jobs_"
base_url_after_2020 <- "https://academicjobs.fandom.com/wiki/Archaeology_"

# starts at 2010-2011
# fix for 2021-22
# base UR

years_to_2019 <- map_chr(2012:2019, ~str_glue('{.x}-{.x +1}'))
years_after_2020 <- map_chr(2020:2022, ~str_glue('{.x}-{.x +1}'))
# though it seems to start at 2007-8: https://academicjobs.fandom.com/wiki/Archaeology_07-08

# make a set of URLs for each page for each year
urls_for_each_year <- c(str_glue('{base_url_to_2019}{years_to_2019}'), 
                        str_glue('{base_url_after_2020}{years_after_2020}')) %>% 
    str_replace("2021-2022", "2021-22")

library(rvest)

# all years
urls_for_each_year_headers <- 
map(urls_for_each_year,
    ~.x %>% 
      read_html() %>% 
      html_nodes('.mw-headline') %>% 
      html_text())

# keep only headings that are actual jobs, they include the terms:
job_headings <- c("college", "university")

total_number_of_jobs_per_year <- 
  map(urls_for_each_year_headers,
      ~str_subset(tolower(.x),
          paste0(job_headings, collapse = "|")))

total_number_of_jobs_per_year_n <- 
map_int(total_number_of_jobs_per_year, length)

total_number_of_jobs_per_year_tbl <- 
tibble(
  url_to_data_source_e_g_paste_in_url_to_the_jobs_wiki_page = urls_for_each_year,
  total_number_of_jobs_per_year = total_number_of_jobs_per_year_n
)

# count of TT jobs per year from our manual data collection,
# join with our total number of all jobs by scraping
count_of_tt_jobs_per_year_from_our_form <- 
jobdata %>% 
  group_by(url_to_data_source_e_g_paste_in_url_to_the_jobs_wiki_page) %>% 
  tally() %>% 
  right_join(total_number_of_jobs_per_year_tbl) %>% 
  rename(n_tt_jobs  = n,
         n_total_jobs = total_number_of_jobs_per_year) %>% 
  mutate(n_non_tt_jobs = n_total_jobs - n_tt_jobs,
         ratio_tt_2_ntt = n_tt_jobs / n_non_tt_jobs) %>% 
  mutate(year = str_extract(url_to_data_source_e_g_paste_in_url_to_the_jobs_wiki_page,  "[[0-9]]{4}-[[0-9]]{4}|2021-22")) %>%  
  mutate(year = ifelse(year =="2021-22",  "2021-2022", year)) 

# draw plot
fig_ratio_tt_2_ntt_jobs_per_year <- 
  ggplot(count_of_tt_jobs_per_year_from_our_form) +
  aes(year, 
      group = 1,
      ratio_tt_2_ntt) +
  geom_line(linewidth = 2) +
  geom_hline(yintercept = 1,
             colour = "red") +
  annotate("text", 
           x = 3, 
           y = 1.3, 
           label = "1:1 ratio",
           colour = "red") +
  labs(y = "Ratio of tenure-track\nto non-tenure track and other",
       x = "") +
  theme_minimal(base_size = 14) +
  scale_x_discrete(name = "Academic Year") 
   #guides(x = "none")
  

```


```{r}
# save these three plots as one set
library(cowplot)
plot_grid(
  fig_how_many_jobs_per_year,
  fig_prop_by_job_title_per_year,
  fig_ratio_tt_2_ntt_jobs_per_year,

  ncol = 1,
  align = "hv",
  axis = "lr",
  labels = "AUTO"
)

ggsave(here("analysis",
            "figures", 
            "fig-panel-per-year.png"),
       bg ="white",
       h = 11, # experiment with h and w to get the right size and proportion 
       w = 20,
       units = "in",
       dpi = 900) # make the image nice and crisp

```


```{r}
#| label: fig-requirements-over-time

# look at only these requirements because the others are flat

intresting_requirements <- 
c("cover letter",
  "cv",
  "names of recommenders",
  "diversity statement",
  "research statement",
  "teaching statement")

jobdata_requirements <- 
jobdata %>% 
  select(year_ad_posted,
         starts_with("documents_requested")) %>% 
  pivot_longer(-year_ad_posted) %>% 
  mutate(value = case_when(
    value == "Not requested in the job ad" ~ 0,
    value == "One" ~ 1,
    value == "Two (e.g. two syllabi)" ~ 2,
    value == "Three" ~ 3,
    .default = 0
  ))  %>% 
  # trim names a bit
  mutate(name = str_remove(name, "documents_requested_")) %>% 
  mutate(name = str_replace_all(name, "_", " ")) %>% 
  filter(name %in% intresting_requirements) %>% 
  mutate(name = str_wrap(name, 10),
         year_ad_posted = str_replace(year_ad_posted, "-", "\n"))

jobdata_requirements_means <- 
jobdata_requirements %>% # average number requested per year
  group_by(year_ad_posted, 
           name) %>% 
  summarise(mean_n = mean(value))

integer_breaks <- function(n = 5, ...) {
  fxn <- function(x) {
    breaks <- floor(pretty(x, n, ...))
    names(breaks) <- attr(breaks, "labels")
    breaks
  }
  return(fxn)
}


ggplot(jobdata_requirements_means) +
  aes(year_ad_posted, 
      mean_n,
      group = name) +
  geom_smooth(linewidth = 2,
              colour = "black") +
  geom_jitter(data = jobdata_requirements,
             aes(year_ad_posted, 
                 value),
             alpha = 0.1,
             height = 0.2,
             width =  0.1) +
  facet_wrap(~name,
             scales = "free_y",
             nrow = 1) +
  xlab("Year") +
  ylab("Number requested in job ad") +
  scale_y_continuous(breaks = integer_breaks()) +
  theme_minimal(base_size = 14) +
  theme(axis.text.x = element_text(size = 8),
        strip.text = element_text( size = 20))

ggsave(here("analysis",
            "figures", 
            "fig-requirements-per-year.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion 
       w = 20,
       units = "in",
       dpi = 900) # make the image nice and crisp
  
```



```{r}
#| label: fig-requirements-by-position 

# do the requirements differ for associate positions 
jobdata_requirements_by_rank <- 
jobdata %>% 
  mutate(position_title = case_when(
    str_detect(title_of_position_tenure_track_jobs_only, 
               "associate") ~ "associate",
    str_detect(title_of_position_tenure_track_jobs_only, 
               "assistant") ~ "assistant",
    str_detect(title_of_position_tenure_track_jobs_only, 
               "full") ~ "full"))  %>% 
  select(position_title,
         starts_with("documents_requested")) %>% 
  pivot_longer(-position_title) %>% 
  mutate(value = case_when(
    value == "Not requested in the job ad" ~ 0,
    value == "One" ~ 1,
    value == "Two (e.g. two syllabi)" ~ 2,
    value == "Three" ~ 3,
    .default = 0
  )) %>% 
  filter(!is.na(position_title)) 

jobdata_requirements_by_rank_means <- 
  jobdata_requirements_by_rank %>% 
  group_by(position_title,
           name) %>% 
  summarise(mean = mean(value))

ggplot() +
    geom_jitter(data = jobdata_requirements_by_rank,
                  aes(position_title, 
                      value),
                height = 0.05,
                alpha = 0.1) +
    geom_point(data = jobdata_requirements_by_rank_means,
               aes(position_title,
                   mean),
               size = 4,
               colour = "red") +
  facet_wrap( ~ name,
              scales = "free_y") +
  theme_minimal()
```


```{r}
#| label: fig-geographic-focus-by-year 

# geographic focus by year

library(googlesheets4)
library(stringi)

geographic_foci <-
read_sheet("https://docs.google.com/spreadsheets/d/1AHq49pIyChcgJ7rawe6KMWkdIBXydCamvg8Jslob8Ec/edit#gid=0", sheet = "geography")

geographic_foci_clean <-
  map(
   str_split(geographic_foci$`From the data`, ";"),
   ~.x %>%
     str_squish() %>%
     stri_remove_empty())

jobdata_geo <-
  jobdata %>%
  select(geographic_focus_of_position)

jobdata_geo <-
  # add one column for each geo region in our categories
cbind(jobdata_geo,
      setNames( lapply(geographic_foci$Category2, function(x) x=NA),
                geographic_foci$Category2) )

for(i in 1:length(geographic_foci$Category2)){

  this_location <- geographic_foci$Category2[i]

  # create the pattern to search for
  x <- paste0(geographic_foci_clean[[i]], collapse = "|")

  # do the search through all the job ads for that pattern
  y <- str_detect(jobdata_geo$geographic_focus_of_position,
             x)

  # assign back to our data frame in the appropriate location column
  jobdata_geo[, this_location] <- y

}

# BM TODO: check for job ads that have a location, but we're not getting it

united_states_regions <-
str_subset(geographic_foci$Category2, "US")

jobdata_geo_year <-
jobdata %>%
  bind_cols(jobdata_geo) %>%
  select(year_ad_posted,
         geographic_foci$Category2) %>%
  pivot_longer(-year_ad_posted) %>%
  drop_na()

# how many times each location mentioned?
jobdata_geo_year %>%
  group_by(name) %>%
  summarise(n = sum(value)) %>%
  arrange(desc(n)) %>%
  ggplot() +
  aes(reorder(name, n),
      n)+
  geom_col() +
  xlab("") +
  theme_minimal() +
  coord_flip()

# explore trends over time. put a point on the max year
jobdata_geo_year_tally <-
jobdata_geo_year %>%
 # exclude those with <20 ads
  filter(!name %in% c("Canada & Arctic",
                      "Oceania",
                      "Southeast US",
                      "Southwest US",
                      "Western US",
                      "Midwest US",
                      "Northeastern US"
                      )) %>%
  group_by(year_ad_posted,
           name) %>%
  summarise(n = sum(value)) %>%
  mutate(prop = n / sum(n))

jobdata_geo_year_tally_max <-
  jobdata_geo_year_tally %>%
  group_by(
           name ) %>%
  filter(prop == max(prop))

library(ggrepel)

ggplot() +
  geom_smooth(data =  jobdata_geo_year_tally,
            aes(year_ad_posted,
                 prop,
                 group = name,
                 colour = name),
            size = 3,
           se = FALSE 
           ) +
  xlab("Year") +
  ylab("Proportion of all ads") +
  scale_colour_brewer(palette = "Dark2") +
  guides(colour = guide_legend("Geographic\nfocus",
                               label.position = "bottom")) +
    theme_minimal( base_size = 14) +
  theme(legend.position="bottom") 

ggsave(here("analysis",
            "figures", 
            "fig-geo-focus-by-year.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion 
       w = 12,
       units = "in",
       dpi = 900) # make the image nice and crisp)


# what about within the US
# how many times each location mentioned?
jobdata_geo_year %>%
  group_by(name) %>%
  summarise(n = sum(value)) %>%
  arrange(desc(n)) %>%
  filter(name %in% united_states_regions) %>% 
  ggplot() +
  aes(reorder(name, n),
      n)+
  geom_col() +
  ylab("Number of ads") +
  xlab("") +
  theme_minimal(base_size = 24) +
  coord_flip()

ggsave(here("analysis",
            "figures", 
            "fig-geo-us-focus-by-year.png"),
       bg ="white",
       h = 5, # experiment with h and w to get the right size and proportion 
       w = 6,
       units = "in",
       dpi = 900) # make the image nice and crisp)


```




```{r}

# topical focus by year

library(googlesheets4)
library(stringi)

topical_foci <-
  read_sheet("https://docs.google.com/spreadsheets/d/1AHq49pIyChcgJ7rawe6KMWkdIBXydCamvg8Jslob8Ec/edit#gid=0",
             sheet = "topic")

topical_foci_clean <-
  map(
    str_split(topical_foci$`From the data`, ";"),
    ~.x %>%
      str_squish() %>%
      stri_remove_empty() %>%
      str_to_lower)

jobdata_topic <-
  jobdata %>%
  select(topical_focus_of_position) %>%
  mutate(topical_focus_of_position = str_to_lower(topical_focus_of_position))

jobdata_topic <-
  # add one column for each topic in our categories
  cbind(jobdata_topic,
        setNames( lapply(topical_foci$Category, function(x) x=NA),
                  topical_foci$Category) )

for(i in 1:length(topical_foci$Category)){

  this_topic <- topical_foci$Category[i]

  # create the pattern to search for
  x <- paste0(topical_foci_clean[[i]], collapse = "|")

  # do the search through all the job ads for that pattern
  y <- str_detect(jobdata_topic$topical_focus_of_position,
                  x)

  # assign back to our data frame in the appropriate location column
  jobdata_topic[, this_topic] <- y

}

jobdata_topic_year <-
  jobdata %>%
  bind_cols(jobdata_topic) %>%
  select(year_ad_posted,
         topical_foci$Category) %>%
  pivot_longer(-year_ad_posted) %>%
  drop_na()

# how many times each topic mentioned?
jobdata_topic_year %>%
  group_by(name) %>%
  summarise(n = sum(value)) %>%
  arrange(desc(n)) %>%
  ggplot() +
  aes(reorder(name, n),
      n)+
  geom_col() +
  xlab("") +
  theme_minimal() +
  coord_flip()

# explore trends over time. put a point on the max year
jobdata_topic_year_tally <-
  jobdata_topic_year %>%
  # exclude those with <20 ads
  filter(!name %in% c("Digital Archaeology",
                      "Pleistocene archaeology",
                      "Mesoamerican Archaeology",
                      "Biological anthropology",
                      "Archaeological theory",
                      "Evolutionary anthropology",
                      "North American archaeology"
  )) %>%
  group_by(year_ad_posted,
           name) %>%
  summarise(n = sum(value)) %>%
  mutate(prop = n / sum(n))

jobdata_topic_year_tally_max <-
  jobdata_topic_year_tally %>%
  group_by(
    name ) %>%
  filter(prop == max(prop))

library(ggrepel)

ggplot() +
  geom_smooth(data =  jobdata_topic_year_tally,
            aes(year_ad_posted,
                prop,
                group = name,
                colour = name),
            size = 3,
           span = 0.7,
            se = FALSE) +
  xlab("Year") +
  ylab("Proportion of all ads") +
  scale_colour_brewer(palette = "Dark2") +
  guides(colour = guide_legend("Topic\nfocus",
                               label.position = "bottom")) +
    theme_minimal( base_size = 14) +
  theme(legend.position="bottom") 

ggsave(here("analysis",
            "figures", 
            "fig-topic-focus-by-year.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion
       w = 12,
       units = "in",
       dpi = 900) # make the image nice and crisp))

```


```{r}
# Draw of map to show which states have done the most hiring in our sample

# get the text in parentheses after the university name that gives the
# state or country abb
uni_state_country <- # 550
jobdata %>%  # 550 rows
 select(name_of_hiring_university) %>%
  mutate(state_country = regmatches(name_of_hiring_university,
                                    gregexpr( "(?<=\\().+?(?=\\))",
                                              name_of_hiring_university,
                                              perl = T))) %>%
  unnest(state_country)

# tally to get counts:
uni_state_country_tally <-
uni_state_country %>%
  group_by(state_country) %>%
  tally(sort = TRUE)

# did we get all the job ads?
# sum(uni_state_country_tally$n) # 550 all of them

state_to_st <- function(x){
       c(state.abb, 'DC')[match(x, c(state.name, 'District of Columbia'))]
}

state_name_and_abb <-
enframe(state.name, value = 'state_name') %>%
       mutate(state_abbr = state_to_st(state_name))

# filter to get US states only
uni_state_country_tally_us <-
uni_state_country_tally %>%
  filter(state_country %in% state.abb) %>%
  select(state_abbr = state_country, n) %>%
  # make sure we have all states in the dataframe
  # even those with no jobs
  right_join(state_name_and_abb) %>%
  select(state = state_name, n, state_abbr) %>%
  mutate(state = tolower(state)) %>%
  mutate(n = ifelse(is.na(n), 0, n))

# how many jobs ads now?
# sum(uni_state_country_tally_us$n) # 433, 78% of the total

library(ggplot2)
library(fiftystater)
library(tidyverse)
library(ggrepel)

data("fifty_states")

ggplot(data= uni_state_country_tally_us,
       aes(map_id = state)) +
  geom_map(aes(fill = n),
           color= "black",
           linewidth = 0.1,
           map = fifty_states) +
  expand_limits(x = fifty_states$long,
                y = fifty_states$lat) +
  coord_map() +
  geom_text_repel(data = fifty_states %>%
              group_by(id) %>%
              summarise(lat = mean(c(max(lat), min(lat))),
                        long = mean(c(max(long), min(long)))) %>%
              mutate(state = id) %>%
              left_join(uni_state_country_tally_us,
                        by = "state"),
            aes(x = long,
                y = lat,
                label = n,
                bg.color = "white",
                bg.r = 0.1),
            force = 0,
            force_pull = 100)+
  scale_x_continuous(breaks = NULL) +
  scale_y_continuous(breaks = NULL) +
  labs(x = "",
       y = "") +
  theme(legend.position = "bottom",
        panel.background = element_blank()) +
  scale_fill_viridis_c()


ggsave(here("analysis",
            "figures",
            "fig-us-state-map.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion
       w = 12,
       units = "in",
       dpi = 900) # make the image nice and crisp))



```

```{r}
# method focus by year

library(googlesheets4)
library(stringi)

method_foci <-
  read_sheet("https://docs.google.com/spreadsheets/d/1AHq49pIyChcgJ7rawe6KMWkdIBXydCamvg8Jslob8Ec/edit#gid=0",
             sheet = "method")

method_foci_clean <-
  map(
    str_split(method_foci$`From the data`, ";"),
    ~.x %>%
      str_squish() %>%
      stri_remove_empty() %>%
      str_to_lower)

jobdata_method <-
  jobdata %>%
  select(methods_focus_of_position) %>%
  mutate(methods_focus_of_position = str_to_lower(methods_focus_of_position))

jobdata_method <-
  # add one column for each topic in our categories
  cbind(jobdata_method,
        setNames( lapply(method_foci$Category, function(x) x=NA),
                  method_foci$Category) )

for(i in 1:length(method_foci$Category)){

  this_method <- method_foci$Category[i]

  # create the pattern to search for
  x <- paste0(method_foci_clean[[i]], collapse = "|")

  # do the search through all the job ads for that pattern
  y <- str_detect(jobdata_method$methods_focus_of_position,
                  x)

  # assign back to our data frame in the appropriate location column
  jobdata_method[, this_method] <- y

}

jobdata_method_year <-
  jobdata %>%
  bind_cols(jobdata_method) %>%
  select(year_ad_posted,
         method_foci$Category) %>%
  pivot_longer(-year_ad_posted) %>%
  drop_na()

# how many times each topic mentioned?
jobdata_method_year %>%
  group_by(name) %>%
  summarise(n = sum(value)) %>%
  arrange(desc(n)) %>%
  ggplot() +
  aes(reorder(name, n),
      n)+
  geom_col() +
  xlab("") +
  theme_minimal() +
  coord_flip()

# explore trends over time. put a point on the max year
jobdata_method_year_tally <-
  jobdata_method_year %>%
  # exclude those with <20 ads
  filter(!name %in% c("Material culture analysis"
  )) %>%
  group_by(year_ad_posted,
           name) %>%
  summarise(n = sum(value)) %>%
  mutate(prop = n / sum(n))

jobdata_method_year_tally_max <-
  jobdata_method_year_tally %>%
  group_by(
    name ) %>%
  filter(prop == max(prop))

library(ggrepel)

ggplot() +
  geom_smooth(data =  jobdata_method_year_tally,
            aes(year_ad_posted,
                prop,
                group = name,
                colour = name),
            size = 3,
           span = 0.7,
            se = FALSE) +
  xlab("Year") +
  ylab("Proportion of all ads") +
  guides(colour = guide_legend("Method\nfocus",
                               label.position = "bottom")) +
    theme_minimal( base_size = 14) +
  theme(legend.position="bottom") 

ggsave(here("analysis",
            "figures", 
            "fig-method-focus-by-year.png"),
       bg ="white",
       h = 10, # experiment with h and w to get the right size and proportion
       w = 12,
       units = "in",
       dpi = 900) # make the image nice and crisp))
```

```{r}
# Carnegie Classification
library(googlesheets4)
library(stringi)

CC <-
  read_sheet( "https://docs.google.com/spreadsheets/d/1Oe_3X-OR7iuUThb6Tp48prBpbHrWjU4osmHKi1Hbqgc/edit#gid=1922578308",
             sheet = "Values1")
Basic2021 <-
  read_sheet( "https://docs.google.com/spreadsheets/d/1Oe_3X-OR7iuUThb6Tp48prBpbHrWjU4osmHKi1Hbqgc/edit#gid=1922578308",
             sheet = "Data1") 
  # remove a bunch of text from uni names to improve our joins
  
uni_name <- # 550
 jobdata %>%  # 550 rows
  # remove parentheses and their contents
  mutate(name = str_squish(str_replace(name_of_hiring_university,
                            "\\s*\\(.*?\\)$", 
                            ""))) %>% 
  mutate(name = str_replace(name,
                            "California State University,[[:space:]]",
                            "California State University-")) %>% 
 
  left_join(Basic2021, 
            keep = TRUE ) 

# how many schools did we match with the CC data
uni_name %>% 
  filter(!is.na(basic2021)) %>% 
  nrow() # 245, 290, 300, 313, getting there!  / 500

# which schools don't match with the CC data?
uni_name %>% 
  select(name.x, name.y)  %>% 
  filter(is.na(name.y)) %>% 
  distinct() %>% View

# for each of these, we need to check
# - In the US or not? If not in the US, we can ignore, if in the US...
# - Compare the uni name in the uni_name data frame to the Basic2021 dataframe. Sometimes there are simple punctuation differences, but most differences seem to be because the Basic2021 data have a location near the university name, which our raw data currently lack. 
# - For example, our spreadsheet we had "Texas A&M University", but in the Basic2021 dataframe we see three possible matches:
# Texas A&M University-Texarkana
# Texas A&M University-San Antonio
# Texas A&M University-Central Texas
# so we need to take a look at the details of the job ad, and find the details to confirm which is the correct match for the "Texas A&M University" in our data. 
# Once we've confirmed the correct match, we update the text in the appropriate row of our raw data, column F of this sheet https://docs.google.com/spreadsheets/d/1Jwe3UqJyedrV-QWlwR_44__t4xBVrCfxGyhXdi3E0sg/edit?resourcekey#gid=1686084773 to match what the university name is in the Basic2021 dataframe. So the full university name in our spreadsheet becomes "Texas A&M University-San Antonio (TX)", then we can match the university name when we join, and we keep the state abbreviation for mapping. I've done this for several schools, can you please continue it?
# - if it's not clear which university is the correct match to our data, choose the one with the highest CC rank. For example, for three U of Texas campuses, the ranks are 19, 18, 19. I could not confirm from the details of the job ad, so I choose Texas A&M University-San Antonio because it has the rank of 18, higher than the other two. It's most likely that the biggest and higher ranking campus will hire an archaeology professor, because smaller campuses tend not to have archaeologists. 
# - then download our raw data as CSV, move it into this repo, and re-run the code to read it in and join with the CC data, and see how many more jobs match with the CC school names. On our SAA poster we say we have 433 US jobs, so we should be able to match around that many. The rest are international unis that wont be in the CC data 



```


# Discussion

# Conclusion

# Acknowledgements

<!-- The following line inserts a page break  -->

\newpage

# References

<!-- The following line ensures the references appear here for the MS Word or HTML output files, rather than right at the end of the document (this will not work for PDF files):  -->

::: {#refs}
:::

\newpage

### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies:

```{r}
#| label: colophon
#| cache: false

# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())  
```
